\begin{proposition}[Proposition 4.1 in \cite{Kemp2013RMTNotes}]
  \label{prop:matrix_moments_convergence}
  \notready
  \uses{def:}
  Let $\{Y_{ij}\}_{1\le i\le j}$ be independent random variables, with $\{Y_{ii}\}_{i\ge 1}$ identically distributed and $\{Y_{ij}\}_{1\le i<j}$ identically distributed.  Suppose that $r_k = \max\{\bE(|Y_{11}|^k),\bE(|Y_{12}|^k)\} <\infty$ for each $k\in\bN$.  Suppose further than $\bE(Y_{ij})=0$ for all $i,j$ and set $t=\bE(Y_{12}^2)$.  If $i>j$, define $Y_{ij} \equiv Y_{ji}$, and let $\mathbf{Y}_n$ be the $n\times n$ matrix with $[\mathbf{Y}_n]_{ij} = Y_{ij}$ for $1\le i,j\le n$.  Let $\mathbf{X}_n = n^{-1/2}\mathbf{Y}_n$ be the corresponding Wigner matrix.  Then
\[
\lim_{n\to\infty} \frac{1}{n}\bE\Tr(\mathbf{X}_n^k) = \begin{cases}
  t^{k/2}C_{k/2}, & k\text{ even} \\
  0, & k\text{ odd}
\end{cases}.
\]
\end{proposition}

\begin{proof}
\notready
Proof
\end{proof}

\iffalse

\begin{lemma}\label{lem:trace_smul}
  %\uses{????}
  \mathlibok %what does this mean
  \lean{Matrix.trace_smul}
  Let $R$ be a ring with a monoid action from $\alpha$ (i.e., $\alpha$ acts distributively on $R$).
  For any scalar $r \in \alpha$ and any square matrix $A$ over $R$, the trace of the scalar
  multiple $r \cdot A$ equals the scalar multiple of the trace of $A$, i.e.,
  \[ \text{tr}(r \cdot A) = r \cdot \text{tr}(A). \]
  \end{lemma}

  \begin{proof}\leanok %what does this mean
  %idk what to put in here
  \end{proof}

\begin{definition}[Definition 4.2 in \cite{Kemp2013RMTNotes}]
  \label{def:graph}
  \lean{graph}
  \maybelean
  \notready
  Let $\mathbf{i} = (i_1,...,i_k) \in [n]^k$ be a $k$-index. Define a graph $G_\mathbf{i}$ as follows: the
  vertices $V_\mx{i}$ are the distinct elements of $\{i_1,...,i_k\}$, and the edges $E_\mathbf{i}$ are
  the distinct pairs among $\{i_1,i_2\},\{i_2,i_3\},...,\{i_{k-1},i_k\},\{i_k,i_1\}$. The path
  $w_\mathbf{i}$ is the sequence
  $$w_\mathbf{i} = \bigl( \{i_1,i_2\},\{i_2,i_3\},...,\{i_{k-1},i_k\},\{i_k,i_1\} \bigl)$$
  of edges.
\end{definition}

% 4.2
% lemma (lemma1): using Matrix.mul_apply?? + induction?


% definition of trace
% linearity of expectation --> map_expect????
% lemma1
% defn of Y_i, [n].


% define a graph (no idea how to do this in lean)

% matrices commutative? don't really understand this part of the proof

%define \omega ({i, j})



%---Richard's Part (Start)---% 


% For any $k$-index $\mathbf{i}$, the connected *oriented* graph $G_\mathbf{i}$ has at most $k$ vertices. 
% Since $w_\mathbf{i}$ records all the non-zero exponents (which sum to the number of terms $k$) in $Y_\mathbf{i}$,
%we have $|w_\mathbf{i}| \equiv \sum_{e \in E_\mathbf{i}} w_\mathbf{i}(e) = k$.
% Motivated by these conditions, we define $\mathcal{G}_k$:


\begin{definition}[Definition 4.2 in \cite{Kemp2013RMTNotes}]
  \label{def:}
  \notready
  \uses{}
  Let $\mathcal{G}_k$ denote the set of all pairs $(G,w)$ where $G = (V,E)$ is a connected graph with at most $k$ vertices, and
  $w$ is a closed walk covering $G$ satisfying $|w| = k$.
\end{definition}

% Using Equation \ref{* *}, we can re-index the sum of Equation \ref{* *} as

% $$ \bE \Tr (\mathbf{Y}_\mathbf{i}^k) 
%= \sum_{(G,w) \in \mathcal{G}_k} \sum_{\substack{\mathbf{i} \in [n]^k \\ (G_\mathbf{i},w_\mathbf{i}) = (G,w)}} \bE (Y_\mathbf{i})
%= \sum_{(G,w) \in \mathcal{G}_k} \Pi (G,w) \cdot \# \{ \mathbf{i} \in [n]^k : (G_\mathbf{i},w_\mathbf{i}) = (G,w) \}.$$

% Combining with renormalization of Equation \ref{* *}, we get

% $$ \frac{1}{n} \bE \Tr (\mathbf{X}_n^k) = \sum_{(G,w) \in \mathcal{G}_k} \Pi (G,w) \cdot \frac{\# \{ \mathbf{i} \in [n]^k : (G_\mathbf{i},w_\mathbf{i}) = (G,w) \} }{n^{k/2+1}}.$$

% It is, in fact, quite easy to count the set of $k$-indexes in Equation \ref{* *}. 
% For any $(G,w) \in \mathcal{G}_k$, an index with that corresponding graph $G$ and walk $w$ is completely determined by assigning which distinct values of $[n]$ appear at the vertices of $G$.
% In general, we have:

\begin{lemma}[Lemma 4.3 in \cite{Kemp2013RMTNotes}]
  \label{lem:}
  \notready
  \uses{}
  Given $(G,w) \in \mathcal{G}_k$, denote by $|G|$ the number of vertices in $G$. Then
  $$\# \{ \mathbf{i} \in [n]^k : (G_\mathbf{i},w_\mathbf{i}) = (G,w) \} = n (n-1) \cdots (n - |G| + 1).$$ 
\end{lemma}

\begin{proof}
  \notready
  Given two $k$-indexes $\mathbf{i}$ and $\mathbf{i}^*$, we first show that they generate *isomorphic* graphs $G_\mathbf{i} = (V_\mathbf{i},E_\mathbf{i})$ and $G_{\mathbf{i}^*} = (V_{\mathbf{i}^*},E_{\mathbf{i}^*})$ 
  if and only if $|G_\mathbf{i}| = |G_{\mathbf{i}^*}|$.
  The right direction directly follows from the definition of graph isomorphism. 
  To prove the left direction, suppose $|G_\mathbf{i}| = m = |G_{\mathbf{i}^*}|$.
  Let us order the distinct entries of $\mathbf{i}$ as $i_{\lambda_1},i_{\lambda_2},...,i_{\lambda_m}$, 
  so that they appear in $\mathbf{i}$ in an ascending order (e.g. $\mathbf{i} = (i_{\lambda_1},i_{\lambda_1},i_{\lambda_2},...,i_{\lambda_m},i_{\lambda_m})$).
  Similarly, we can order the distinct elements of $\mathbf{i}^*$ by the same criterion as $i_{\lambda_1}^*,i_{\lambda_2}^*,...,i_{\lambda_m}^*$.
  Then we can establish a bijection $\varphi : V_\mathbf{i} \rightarrow V_{\mathbf{i}^*}$ defined by $\varphi(i_{\lambda_l})=i_\{\lambda_l}^*$ for each $l = 1,...,m$.
  This implies
  $$\{ i_{\lambda_{l_1}},i_{\lambda_{l_2}} \} \in E_\mathbf{i} \Longleftrightarrow \{ \varphi(i_{\lambda_{l_1}}),\varphi(i_{\lambda_{l_2}}) \} \in E_{\mathbf{i}^*}.$$
  In other words, the entries $i_{\lambda_{l_1}}$ and $i_{\lambda_{l_2}}$ are adjacent entires in $\mathbf{i}$ if and only if $\varphi(i_{\lambda_{l_1}})$ and $\varphi(i_{\lambda_{l_2}})$ are adjacent entries in $\mathbf{i}^*$.
  Note that we are also treating $i_{\lambda_1}$ and $i_{\lambda_m}$ as adjacent entries in $\mathbf{i}$ and $i_{\lambda_1}^*$ and $i_{\lambda_m}^*$ as adjacent entries in $\mathbf{i}^*$.
  The fact that there are $n (n - 1) \cdots (n - |G| + 1)$ ways to assign $|G| = m$ distinct values from $[n]$ into the indices $i_1,...,i_m$ completes the proof.
\end{proof}

% By Lemma \ref{* *}, Equation \ref{* *} can be reduced to

% $$ \frac{1}{n} \bE \Tr (\mathbf{X}_n^k) = \sum_{(G,w) \in \mathcal{G}_k} \Pi (G,w) \cdot \frac{n (n-1) \cdots (n - |G| + 1)}{n^{k/2+1}}.$$

% Note that the summation is finite, and thus we only need to determine the values of $\Pi (G,w)$ to evaluate the summation. 
% We begin with a simple observation: let $(G,w) \in \mathcal{G}_k$ and suppose there exists an edge $e = \{i,j\}$ with $w(e) = 1$.
% This means, by Equation \ref{* *}, a singleton term $\bE (Y_{ij}^{w(e)}) = \bE (Y_{ij})$ appears.
% By assumption, the variables $Y_{ij}$ are all centered and hence the product $\Pi (G,w) = 0$ for any such pair $(G,w)$.
% This considerably reduces the sum in Equation \ref{* *} since we only need to consider those $w$ that cross each edge at least twice.
% We record this condition as $w \geq 2$, so

% $$ \frac{1}{n} \bE \Tr (\mathbf{X}_n^k) = \sum_{\substack{(G,w) \in \mathcal{G}_k \\ w \geq 2}} \Pi (G,w) \cdot \frac{n (n-1) \cdots (n - |G| + 1)}{n^{k/2+1}}.$$

% Thus, the condition $w \geq 2$ restricts those graphs that can appear. 
% Since $|w_\mathbf{i}| = k$, if each edge in $G_\mathbf{i}$ is traversed at least twice, this means that the number of edges is $\leq k/2$.


%---Richard's Part (End)---% 




% 4.4: true because multiplication is commutative (EReal.mul_comm)


% more graph definitions (idk how lean would work)


% smt smt get to 4.5

% 4.6: use 4.5, scalar multiple of expectatio (Finset.smul_expect), trace of scalar multiple (Matrix.trace_smul)


% lemma 4.3: lots more work needed

%4.7: use lemma 4.3

% w >= 2 stuff
% 4.8

% 4.3.1: maybe SimpleGraph.IsTree.card_edgeFinset??

% test edit by Richard
%% test edit 2 by Richard

\fi
